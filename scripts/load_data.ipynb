{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Kopie von Copy of Copia di Tutorial_notebook.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.11"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8vyO72L0J9PP"
      },
      "source": [
        "# **LOAD DATA - Do people with different ideologies speak differently?**\n",
        "*ADA Project Milestone P2*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oA_fe4oxWT6p"
      },
      "source": [
        "# Mouting the Google Drive\n",
        "\n",
        "It is possible to mount your Google Drive to Colab if you need additional storage or if you need to use files from it. To do that run (click on play button or use keyboard shortcut 'Command/Ctrl+Enter') the following code cell:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CEcIlRwfWY4C"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Nfr-aXhWbFk"
      },
      "source": [
        " \n",
        "\n",
        "1.   After running the cell, URL will appear.\n",
        "\n",
        "2.   Following this URL, you will be redirected to the page where you need to choose Google Drive account to mount to.\n",
        "\n",
        "3.   You will further be asked to give Google Drive Stream a permission to access the chosen Google account\n",
        "\n",
        "4.   After granting the access, authorization code will be given to you\n",
        "\n",
        "5.   Copy the authorization code into the dedicated textbox in Colab under '*Enter your authorization code:*' writing\n",
        "\n",
        "After copying the authorization code, you should get the message saying '*Mounted at /content/gdrive*'\n",
        "\n",
        "Path to the files from the mounted Drive will then be '/content/drive/MyDrive/'. By opening the Files tab (left sidebar, folder icon) you should also be able to see the accessible files."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLRO9Z9RUguz"
      },
      "source": [
        "# Required libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uYboyMBwhBL1"
      },
      "source": [
        "!pip install pandas==1.0.5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0M9XTehUfcA"
      },
      "source": [
        "# Imports\n",
        "import bz2\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.stats as stats"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERh-AV9gC602"
      },
      "source": [
        "!pip install -q condacolab\n",
        "import condacolab\n",
        "condacolab.install()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFOKf71SU36O"
      },
      "source": [
        "# Load/filter/merge quotebank and wikidata samples\n",
        "In this section 5 main operations are performed:\n",
        "- **load and filter wikidata** from a parquet file into a data frame. The filtering is based on our initial goal, so based on polytical parties;\n",
        "- **load quotebank dataset** from json bz2 file chunk by chunk, technique chosen to address the initial file size;\n",
        "- **filter quotebank** entries\n",
        "- **merge quotebank and wikidata** on the QID\n",
        "- **store merged df for each year** in an additional parquet file (initially a json bz2 file format was chosen, but the memory requirements needed in order to handle them exceeded the available memory from colab)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tmWhmdfpOeoz"
      },
      "source": [
        "# Load and filter wikidata for our purpose\n",
        "def load_filter_wikidata_df(path_to_file):\n",
        "  # load from file\n",
        "  columns = [\"id\", \"gender\", \"occupation\", \"party\"]\n",
        "  df_wikidata = pd.read_parquet(path_to_file, columns=columns)\n",
        "\n",
        "  # Filter\n",
        "  # Remove rows without a party\n",
        "  df_wikidata_parties = df_wikidata.dropna(subset=['party'])\n",
        "\n",
        "  # Select only rows with either republican or democrats party\n",
        "  QID_republicans = \"Q29468\"\n",
        "  QID_democrats = \"Q29552\"\n",
        "  df_wikidata_filtered = df_wikidata_parties[df_wikidata_parties.apply(lambda x: (QID_republicans in x['party']) or (QID_democrats in x['party']) , axis=1)]\n",
        "\n",
        "  return df_wikidata_filtered"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o93OsnXfUu58"
      },
      "source": [
        "# Perform all operations needed on quotebank dataset (load/filter/merge with wikidata/store)\n",
        "def handle_quotebank_df(input_file, chunk_size, df_wikidata, n_chunks=0):\n",
        "  curr_chunk = 0\n",
        "  df = pd.DataFrame()\n",
        "\n",
        "  # read input file by chunks (as the whole file can't fit into memory)\n",
        "  reader = pd.read_json(input_file, lines=True, compression='bz2', chunksize=chunk_size)\n",
        "  for chunk in reader:\n",
        "    #if curr_chunk == n_chunks:\n",
        "     # break\n",
        "    curr_chunk += 1\n",
        "    # append only when the speaker is knows (the best % is not from \"None\" speaker)\n",
        "    chunk = chunk[chunk['speaker'] != 'None' ][['quoteID', 'quotation','speaker', 'qids', 'probas']]\n",
        "    \n",
        "    # apply filter to single chunk\n",
        "    chunk = filter_quotebank_df(chunk)\n",
        "\n",
        "    # merge single chunk with wikidata in order to reduce even further the dataset and allow RAM to store it\n",
        "    chunk = merge_quotebank_wikidata_df(chunk, df_wikidata)\n",
        "    \n",
        "    df = pd.concat([df, chunk], ignore_index=True)\n",
        "\n",
        "  return df\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d116TpH2NYEa"
      },
      "source": [
        "# Filter unuset entries in wikidata\n",
        "def filter_quotebank_df(df):\n",
        "  # remove the data with not unique qid speaker because we are not sure who is the speaker: speakers with same name but different qids\n",
        "  df_filtered = df[df.apply(lambda x: len(x['qids']) == 1, axis=1)]\n",
        "\n",
        "  # now we don't have anymore list of quids (only 1 quid per entry possible), so remove list and store only the single value\n",
        "  df_filtered['qids'] = df_filtered['qids'].apply(lambda x: x[0])\n",
        "\n",
        "  return df_filtered"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hnfAScdVNX6i"
      },
      "source": [
        "# Merge quotebank and wikidata entries on QID\n",
        "def merge_quotebank_wikidata_df(df_quotebank, df_wikidata):\n",
        "  #merge quotebank data with wikidata \n",
        "  df_merged = df_quotebank.merge(right=df_wikidata, how='inner', left_on='qids', right_on='id')\n",
        "\n",
        "  #drop the id column because we already have the qid\n",
        "  df_merged = df_merged.drop(labels='id', axis=1)\n",
        "  \n",
        "  return df_merged"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NovbYnVqNXyh"
      },
      "source": [
        "# Print to output file in json compressed format\n",
        "def store_df(path_to_file, df):\n",
        "  # Dump the single chunk to csv, appending it to previously written chunks\n",
        "  df.to_parquet(output_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PmnvsTIx54Bh"
      },
      "source": [
        "Actual data cleaning and preprocessing is done here. The final dataframe for each year is saved in an additional .parquet file.<br>\n",
        "*(Note that it takes around 3 hours to run the following cell)*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZ95GHaNLnbq"
      },
      "source": [
        "chunk_size = 100000\n",
        "# n_chunks = 10\n",
        "\n",
        "path_to_parquet = '/content/drive/MyDrive/Project datasets/speaker_attributes.parquet'\n",
        "df_wikidata = load_filter_wikidata_df(path_to_parquet)\n",
        "\n",
        "input_file = '/content/drive/MyDrive/Quotebank/quotes-2020.json.bz2'\n",
        "output_file = '/content/drive/MyDrive/Quotebank_Repub_Dem/new-quotes-2020-repub-dem.parquet'\n",
        "df = handle_quotebank_df(input_file, chunk_size, df_wikidata)\n",
        "store_df(output_file, df)\n",
        "df = []\n",
        "\n",
        "input_file = '/content/drive/MyDrive/Quotebank/quotes-2019.json.bz2'\n",
        "output_file = '/content/drive/MyDrive/Quotebank_Repub_Dem/new-quotes-2019-repub-dem.parquet'\n",
        "df = handle_quotebank_df(input_file, chunk_size, df_wikidata)\n",
        "store_df(output_file, df)\n",
        "df = []\n",
        "\n",
        "input_file = '/content/drive/MyDrive/Quotebank/quotes-2018.json.bz2'\n",
        "output_file = '/content/drive/MyDrive/Quotebank_Repub_Dem/new-quotes-2018-repub-dem.parquet'\n",
        "df = handle_quotebank_df(input_file, chunk_size, df_wikidata)\n",
        "store_df(output_file, df)\n",
        "df = []\n",
        "\n",
        "input_file = '/content/drive/MyDrive/Quotebank/quotes-2017.json.bz2'\n",
        "output_file = '/content/drive/MyDrive/Quotebank_Repub_Dem/new-quotes-2017-repub-dem.parquet'\n",
        "df = handle_quotebank_df(input_file, chunk_size, df_wikidata)\n",
        "store_df(output_file, df)\n",
        "df = []\n",
        "\n",
        "input_file = '/content/drive/MyDrive/Quotebank/quotes-2016.json.bz2'\n",
        "output_file = '/content/drive/MyDrive/Quotebank_Repub_Dem/new-quotes-2016-repub-dem.parquet'\n",
        "df = handle_quotebank_df(input_file, chunk_size, df_wikidata)\n",
        "store_df(output_file, df)\n",
        "df = []\n",
        "\n",
        "input_file = '/content/drive/MyDrive/Quotebank/quotes-2015.json.bz2'\n",
        "output_file = '/content/drive/MyDrive/Quotebank_Repub_Dem/new-quotes-2015-repub-dem.parquet'\n",
        "df = handle_quotebank_df(input_file, chunk_size, df_wikidata)\n",
        "store_df(output_file, df)\n",
        "df = []\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}